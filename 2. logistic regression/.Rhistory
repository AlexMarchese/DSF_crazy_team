print('Hey guys')
#***********************************************
# LOGISTIC REGRESSIONS FOR BANK LOAND DEFAULTS
#***********************************************
#
# The logistic function is very similar to
# OLS regression, but it is used for CLASSIFICATION problems,
# whereas OLS is used for REGRESSION problems.
# Both are the prototypical "plain vanilla" models
# for each type of problem. There are almost
# always the natural starting point (except when
# you have "non-spreadsheet data" like in vision or
# text understanding).
# In fact, the classification problem for cancer types with
# the WDBC data is, yes, a classification problem. However, when
# deriving weights by minimizing the MSE using gradient
# descent, we have in fact treated it as a regression problem!
# Stronger, the weights we have derived via gradient descent
# are exactly the weights of an OLS regression! This is
# OK, but usually we prefer a different approach to classification
# problems where the models predict a number between 0 and 1
# that can be interpreted as a probability or share. By contrast
# our score (w0 + w1*x1 + w2*x2) can take on any values.
# A logistic regression provides a simple way to convert a
# score into a number between 0 and 1 that is interpreted as
# a probability. To see this, check this:
rm(list = ls())
x = seq(-5, 5, by = 0.01)
y = 1/(1+exp(-x))
plot(x = x, y = y, type = "l", col = "turquoise")
grid()
# Here, we use data on bank loans to illustrate the use of
# logistic regressions, and also for calculating the ROC
# curve. You can try it out with the WDBC data on your own.
# Preparations ------------
# **************************
library(tidyverse)
library(caret)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
D0 = read_csv("../Data/LoanDefaults.csv")
head(D0)
# Our y/target variable is Loan_Status, with 1 indicating no repayment of a loan.
# What about NAs in the data?
table(is.na(D0$Months_since_last_delinquent))
map(D0, function(x) table(is.na(x)))
# Let's drop a column with many NAs, and then all
# rows with NAs.
Dt = D0 %>% select(-Months_since_last_delinquent) %>%
na.omit()
# As you can see, many of the variables have text/character values.
# In a logistic regression, we need numerical values for all
# features (and the target). We could leave the conversion
# to the glm function that we use below. However, it is
# often good practice to convert the data beforehand.
# So let's illustrate one way to do this, i.e. to dummy-code
# these feature values (there are many ways to do so). Here,
# we use the model.matrix function to do this (no need to load extra packages).
D = as_tibble(model.matrix(~ ., data = Dt)) %>%
select(-"(Intercept)")
# What are the new variables and their names?
names(D)
unique(D0$Term)
unique(D0$Home_Ownership)
unique(D0$Purpose)
# The names are OK and easy to understand. If this were a bigger project,
# you may want to use string functions to edit the original
# names and entries in D0. But we are not going to bother here.
# A logistic regression using all data ---------
# **********************************************
# Let's now first estimate a logistic regression on the entire
# data set
# Note the family argument. It's this one that contains the
# instruction to run a logistic regression
modelAll = glm(Loan_Status ~ ., family = "binomial", data = D) #even if binomial
# = logistic regression
dim(D)
predict(modelAll, type = "response")
D = D %>%
mutate(probAll = predict(modelAll, type = "response")) %>%
mutate(predAll50 = ifelse(probAll >=0.5, 1, 0))
# Calculate error measures
misClas_predAll50 = mean(D$predAll50 != D$Loan_Status)
AfalPos_predAll50 =
length(which(D$Loan_Status == 0 & D$predAll50 == 1))/
length(which(D$Loan_Status == 0))
falNeg_predAll50 =
length(which(D$Loan_Status == 1 & D$predAll50 == 0))/
length(which(D$Loan_Status == 1))
View(D)
#***********************************************
# LOGISTIC REGRESSIONS FOR BANK LOAND DEFAULTS
#***********************************************
#
# The logistic function is very similar to
# OLS regression, but it is used for CLASSIFICATION problems,
# whereas OLS is used for REGRESSION problems.
# Both are the prototypical "plain vanilla" models
# for each type of problem. There are almost
# always the natural starting point (except when
# you have "non-spreadsheet data" like in vision or
# text understanding).
# In fact, the classification problem for cancer types with
# the WDBC data is, yes, a classification problem. However, when
# deriving weights by minimizing the MSE using gradient
# descent, we have in fact treated it as a regression problem!
# Stronger, the weights we have derived via gradient descent
# are exactly the weights of an OLS regression! This is
# OK, but usually we prefer a different approach to classification
# problems where the models predict a number between 0 and 1
# that can be interpreted as a probability or share. By contrast
# our score (w0 + w1*x1 + w2*x2) can take on any values.
# A logistic regression provides a simple way to convert a
# score into a number between 0 and 1 that is interpreted as
# a probability. To see this, check this:
rm(list = ls())
x = seq(-5, 5, by = 0.01)
y = 1/(1+exp(-x))
plot(x = x, y = y, type = "l", col = "turquoise")
grid()
# Here, we use data on bank loans to illustrate the use of
# logistic regressions, and also for calculating the ROC
# curve. You can try it out with the WDBC data on your own.
# Preparations ------------
# **************************
library(tidyverse)
library(caret)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
D0 = read_csv("../Data/LoanDefaults.csv")
head(D0)
# Our y/target variable is Loan_Status, with 1 indicating no repayment of a loan.
# What about NAs in the data?
table(is.na(D0$Months_since_last_delinquent))
map(D0, function(x) table(is.na(x)))
# Let's drop a column with many NAs, and then all
# rows with NAs.
Dt = D0 %>% select(-Months_since_last_delinquent) %>%
na.omit()
# As you can see, many of the variables have text/character values.
# In a logistic regression, we need numerical values for all
# features (and the target). We could leave the conversion
# to the glm function that we use below. However, it is
# often good practice to convert the data beforehand.
# So let's illustrate one way to do this, i.e. to dummy-code
# these feature values (there are many ways to do so). Here,
# we use the model.matrix function to do this (no need to load extra packages).
D = as_tibble(model.matrix(~ ., data = Dt)) %>%
select(-"(Intercept)")
# What are the new variables and their names?
names(D)
unique(D0$Term)
unique(D0$Home_Ownership)
unique(D0$Purpose)
# The names are OK and easy to understand. If this were a bigger project,
# you may want to use string functions to edit the original
# names and entries in D0. But we are not going to bother here.
# A logistic regression using all data ---------
# **********************************************
# Let's now first estimate a logistic regression on the entire
# data set
# Note the family argument. It's this one that contains the
# instruction to run a logistic regression
modelAll = glm(Loan_Status ~ ., family = "binomial", data = D) #even if binomial
# = logistic regression
dim(D)
predict(modelAll, type = "response")
D = D %>%
mutate(probAll = predict(modelAll, type = "response")) %>%
mutate(predAll50 = ifelse(probAll >=0.5, 1, 0))
# Calculate error measures
misClas_predAll50 = mean(D$predAll50 != D$Loan_Status)
AfalPos_predAll50 =
length(which(D$Loan_Status == 0 & D$predAll50 == 1))/
length(which(D$Loan_Status == 0))
falNeg_predAll50 =
length(which(D$Loan_Status == 1 & D$predAll50 == 0))/
length(which(D$Loan_Status == 1))
# Can we rely on this? Use cross validation
# Cross validation ----------
# ***************************
# NOTE: Here, we use cross validation not for doing any model selection
# within the class of logistic regressions (e.g. by adding engineered
# features and checking whether they improve the model). Rather, we want
# to estimate the TEST ERROR! we may use this if we compare logistic
# regressions to other models for bank loan repayments, e.g.
# random forests or boosted trees.
# For cross validation, let's change the type of the target variable to factor
# (the caret package expects this)
D$Loan_Status = as.factor(D$Loan_Status)
# is.factor(D$Loan_Status) #to check if it was converted
# This time, let's use 10 folds for cross-validation (it takes a while)
data_ctrl <- trainControl(method = "cv", number = 10)
model_caret <- train(Loan_Status ~ .,   # model to fit
data = D,
trControl = data_ctrl,           # folds
method = "glm",                   # specifying regression model
family = "binomial",
na.action = na.pass)             # A standard setting that you can ignore
model_caret
names(model_caret)
model_caret$finalModel
model_caret$results
# get Accuracy as
model_caret$results[["Accuracy"]]
# Use "final model" for prediction. The name is misleading here
# since we have only one model, we do no comparison here
# but estimate the test error using cross-validation.
D = D %>%
mutate(probCV = predict(model_caret$finalModel, type = "response")) %>%
mutate(predCV = ifelse(probCV >=0.5, 1, 0))
# and get the CV errors
(misClasCV = mean(D$predCV != D$Loan_Status))
misClasCV + model_caret$results[["Accuracy"]]#we should get a numb close to 1 here
falPosCV =
length(which(D$Loan_Status == 0 & D$predCV == 1))/
length(which(D$Loan_Status == 0))
falNegCV =
length(which(D$Loan_Status == 1 & D$predCV == 0))/
length(which(D$Loan_Status == 1))
# What do you conclude if you compare these errors?
D0
map(D0, function(x) table(is.na(x)))
D <- D0 %>%
select(-Months_since_last_delinquent) %>%
na.omit %>%
mutate(Term = as.factor(Term)) %>%
mutate(Home_Ownership = as.factor(Home_Ownership)) %>%
mutate(Purpose = as.factor(Purpose))
D
nrow(D)
percTraining = 0.7
set.seed(48)
forTraining = sample(1:nrow(D), size = percTraining*nrow(D))
Dtrain = D[forTraining, ]
Dtest = D[-forTraining, ]
D = as_tibble(model.matrix(~ ., data = Dtrain)) %>%
select(-"(Intercept)")
D = as_tibble(model.matrix(~ ., data = Dtest)) %>%
select(-"(Intercept)")
modelAll = glm(Loan_Status ~ ., family = "binomial", data = D)
library(tidyverse)
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
DS <- read.delim("../Data/dataset.csv",sep = ";")
View(DS)
DS <- DS %>% select(c(5, 7, 9, 12))
colnames(DS)[2] = "category"
DS_dupl <- DS %>% select(-country)
DS_unique <- unique(DS_dupl)
View(DS_unique)
DS_unique <- unique(DS_dupl)
DS_unique$count <- 0
row <- 1
row <- 1
row <- 1
category<- DS_unique$category[row]
colour<- DS_unique$colour[row]
price<- DS_unique$price[row]
DS_unique$count[row] <- as.integer(count(DS[which(DS$category == category & DS$colour == colour & DS$price == price),]))
DS_unique$count[row]
for (row in 0:nrow(DS_unique)) {
row <- 1
category<- DS_unique$category[row]
colour<- DS_unique$colour[row]
price<- DS_unique$price[row]
DS_unique$count[row] <- as.integer(count(DS[which(DS$category == category & DS$colour == colour & DS$price == price),]))
}
library(tidyverse)
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
DS <- read.delim("../Data/dataset.csv",sep = ";")
DS <- DS %>% select(c(5, 7, 9, 12))
colnames(DS)[2] = "category"
DS_dupl <- DS %>% select(-country)
DS_unique <- unique(DS_dupl)
DS_unique$count <- 0
# class(DS_unique)
# lapply(DS_unique, class)
for (row in 0:nrow(DS_unique)) {
row <- 1
category<- DS_unique$category[row]
colour<- DS_unique$colour[row]
price<- DS_unique$price[row]
DS_unique$count[row] <- as.integer(count(DS[which(DS$category == category & DS$colour == colour & DS$price == price),]))
}
final_DS <- DS_unique
for (row in 0:nrow(DS_unique)) {
row <- 1
category<- DS_unique$category[row]
colour<- DS_unique$colour[row]
price<- DS_unique$price[row]
DS_unique$count[row] <- as.integer(count(DS[which(DS$category == category & DS$colour == colour & DS$price == price),]))
}
final_DS <- DS_unique
library(tidyverse)
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
DS <- read.delim("../Data/dataset.csv",sep = ";")
DS <- DS %>% select(c(5, 7, 9, 12))
colnames(DS)[2] = "category"
DS_dupl <- DS %>% select(-country)
DS_unique <- unique(DS_dupl)
DS_unique$count <- 0
# class(DS_unique)
# lapply(DS_unique, class)
for (row in 0:nrow(DS_unique)) {
row <- 1
category<- DS_unique$category[row]
colour<- DS_unique$colour[row]
price<- DS_unique$price[row]
DS_unique$count[row] <- as.integer(count(DS[which(DS$category == category & DS$colour == colour & DS$price == price),]))
}
final_DS <- DS_unique
View(final_DS)
library(tidyverse)
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
DS <- read.delim("../Data/dataset.csv",sep = ";")
names(DS)
View(DS)
library(tidyverse)
library(caret)
library(nnet) # for multinomial logistic regression
library(glue) # this is a python f string equivalent -> needed for nice output printing
# clean environment and set current wd -> helps to easier work with imported files
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
DS_or <- read.delim("../Data/dataset.csv",sep = ";") # original dataset
med_1 <- median(DS_or$order)
DS_or <- read.delim("../Data/dataset.csv",sep = ";") # original dataset
DS_aggr <- read.csv("../Data/clean_DS.csv") # aggregated dataset
DS_rearr <- read.csv("../Data/rearranged_DS.csv") # rearranged dataset
View(DS_aggr)
View(DS_rearr)
